(** Hidden Markov Models over integer observations.

    Implements discrete Hidden Markov Models for sequence modeling and
    inference. Supports forward-backward algorithms for probability computation,
    Viterbi decoding for finding the most likely state sequence, and Baum-Welch
    (EM) training for parameter estimation.

    {1 Overview}

    An HMM models sequences of integer observations generated by an underlying
    sequence of hidden states. The model is characterized by:
    - Initial state distribution
    - State transition probabilities
    - Emission (observation) probabilities

    All probability distributions are automatically normalized during creation
    to ensure valid probability mass functions.

    {1 Usage}

    Create an HMM with explicit parameters:
    {[
      (* 2 states, 3 possible observations *)
      let init = [|0.6; 0.4|] in
      let transitions = [|
        [|0.7; 0.3|];  (* From state 0 *)
        [|0.4; 0.6|]   (* From state 1 *)
      |] in
      let emissions = [|
        [|0.5; 0.3; 0.2|];  (* State 0 emissions *)
        [|0.1; 0.4; 0.5|]   (* State 1 emissions *)
      |] in
      let hmm = Hmm.create ~init ~transitions ~emissions
    ]}

    Decode the most likely state sequence:
    {[
      let observations = [|0; 1; 2; 1|] in
      let states = Hmm.viterbi hmm observations
      (* Returns: most likely state path *)
    ]}

    Train using Baum-Welch:
    {[
      let training_sequences = [
        [|0; 1; 2|];
        [|1; 2; 1|];
        [|0; 0; 1|]
      ] in
      let trained_hmm = Hmm.baum_welch hmm training_sequences
    ]}

    {1 Implementation Notes}

    All algorithms use log-space arithmetic internally where appropriate to
    prevent numerical underflow. Rows in probability matrices are normalized
    during forward and backward passes to maintain numerical stability. *)

type init = float array
(** Initial state distribution.

    [init.(i)] is the probability of starting in state [i]. Must sum to 1.0
    after normalization. *)

type transitions = float array array
(** State transition probabilities.

    [transitions.(i).(j)] is P(next_state=j | current_state=i). Each row must
    sum to 1.0 after normalization. *)

type emissions = float array array
(** Observation emission probabilities.

    [emissions.(i).(k)] is P(observation=k | state=i). Each row must sum to 1.0
    after normalization. *)

type t
(** Immutable Hidden Markov Model.

    Stores the complete model parameters including number of states and
    observations. *)

val create : init:init -> transitions:transitions -> emissions:emissions -> t
(** [create ~init ~transitions ~emissions] creates an HMM with explicit
    parameters.

    All probability distributions are automatically normalized to sum to 1.0.
    Invalid probabilities (negative or all zeros) are replaced with uniform
    distributions.

    @raise Invalid_argument if [init] is empty. *)

val num_states : t -> int
(** [num_states hmm] returns the number of hidden states. *)

val num_observations : t -> int
(** [num_observations hmm] returns the number of possible observation values.

    Observations must be integers in the range [0, num_observations - 1]. *)

val log_likelihood : t -> int array -> float
(** [log_likelihood hmm observations] computes the log-probability of the
    observation sequence.

    Uses the forward algorithm to sum over all possible state paths. Returns the
    natural logarithm of P(observations | hmm).

    Empty observation sequences have log-likelihood 0.0. *)

val viterbi : t -> int array -> int array
(** [viterbi hmm observations] finds the most likely hidden state sequence.

    Uses the Viterbi algorithm to compute the maximum likelihood state path.
    Time complexity is O(T * N^2) where T is sequence length and N is the number
    of states.

    Returns an empty array if [observations] is empty.

    {4 Example}

    {[
      let states = Hmm.viterbi hmm [|0; 1; 2|] in
      (* states.(i) is the most likely state at time i *)
    ]} *)

val forward : t -> int array -> float array array
(** [forward hmm observations] computes forward probabilities.

    Returns a matrix where [alpha.(t).(i)] = P(o_0..o_t, state_t = i), the
    probability of observing the sequence up to time t and being in state i at
    time t.

    Probabilities are normalized at each time step to prevent numerical
    underflow. Returns an empty array if [observations] is empty. *)

val backward : t -> int array -> float array array
(** [backward hmm observations] computes backward probabilities.

    Returns a matrix where [beta.(t).(i)] = P(o{_ t+1}..o{_ T} | state{_ t} =
    i), the probability of observing the remaining sequence given state i at
    time t.

    Probabilities are normalized at each time step. Returns an empty array if
    [observations] is empty. *)

val baum_welch : ?tol:float -> ?max_iter:int -> t -> int array list -> t
(** [baum_welch hmm sequences] reestimates HMM parameters using the Baum-Welch
    algorithm.

    Performs Expectation-Maximization to find maximum likelihood parameters
    given the observation sequences. The algorithm iteratively refines the
    initial, transition, and emission probabilities.

    @param tol Convergence tolerance on log-likelihood change. Default: 1e-4.
    @param max_iter Maximum number of EM iterations. Default: 100.

    Returns a new HMM with updated parameters. The input [hmm] provides initial
    parameters and model structure (number of states and observations).

    Convergence occurs when the change in total log-likelihood across all
    sequences falls below [tol], or when [max_iter] is reached.

    {4 Example}

    {[
      let sequences = [[|0; 1; 2|]; [|1; 2; 1|]] in
      let trained = Hmm.baum_welch ~max_iter:50 initial_hmm sequences in
      let ll = Hmm.log_likelihood trained [|0; 1; 2|]
    ]} *)
