<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>kaun - Neural networks for OCaml | raven</title>
  <link rel="stylesheet" href="/styles.css">
</head>
<body>
  <main class="main-content">
    <nav class="kaun-nav nav-breadcrumb">
      <a href="/">raven</a> / kaun <span class="color-red rune-symbol">&#5810;</span>
      [ <a href="/docs/kaun/">docs</a> |
      <a href="https://github.com/raven-ml/raven/tree/main/kaun">source</a> ]
    </nav>

    <div class="kaun-hero hero">
      <h1>kaun <span style="font-size: 48px; margin-left: -20px; opacity: 0.6;" class="color-red">&#5810;</span></h1>
      <p class="tagline">PyTorch's ease. Flax's modularity. OCaml's type safety.</p>
    </div>

    <hr>

    <h2>why kaun?</h2>

    <div class="feature-grid">
      <div>
        <h3 class="color-red">composable layers</h3>
        <p>Layers are values. Compose with <code>sequential</code> and <code>compose</code>. No classes, no inheritance, no magic.</p>
      </div>
      <div>
        <h3 class="color-red">parameter trees</h3>
        <p>All parameters live in a <code>Ptree.t</code>. Inspect, serialize, map, and transform them as plain data structures.</p>
      </div>
      <div>
        <h3 class="color-red">high-level training</h3>
        <p><code>Train.fit</code> composes model, optimizer, data, and loss into a single call. Drop down to <code>Train.step</code> when you need control.</p>
      </div>
      <div>
        <h3 class="color-red">huggingface integration</h3>
        <p>Download pretrained weights from the HuggingFace Hub. SafeTensors checkpointing built in.</p>
      </div>
    </div>

    <hr>

    <h2>show me the code</h2>

    <div class="code-compare">
      <div>
        <h4>PYTORCH</h4>
        <pre class="language-python"><code class="language-python">import torch
import torch.nn as nn

# Model
model = nn.Sequential(
    nn.Linear(784, 128),
    nn.ReLU(),
    nn.Linear(128, 10),
)

# Optimizer
optimizer = torch.optim.Adam(
    model.parameters(), lr=1e-3
)

# Training step
for x, y in dataloader:
    logits = model(x)
    loss = F.cross_entropy(logits, y)
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()</code></pre>
      </div>
      <div>
        <h4>KAUN</h4>
        <pre class="language-ocaml"><code class="language-ocaml">open Kaun

(* Model *)
let model = Layer.sequential [
  Layer.linear ~in_features:784
    ~out_features:128 ();
  Layer.relu ();
  Layer.linear ~in_features:128
    ~out_features:10 ();
]

(* Trainer = model + optimizer *)
let trainer = Train.make ~model
  ~optimizer:(Optim.adam
    ~lr:(Optim.Schedule.constant 1e-3) ())

(* Train over data pipeline *)
let st = Train.fit trainer st data</code></pre>
      </div>
    </div>

    <hr>

    <h2>training in practice</h2>

    <p>A complete training pipeline: data loading, batching, training loop with reporting, and evaluation.</p>

    <pre class="language-ocaml"><code class="language-ocaml">open Kaun

let () =
  let rngs = Rune.Rng.key 42 in
  let dtype = Rune.float32 in

  (* Data pipeline: shuffle, batch, attach loss *)
  let train_data =
    Data.prepare ~shuffle:rngs ~batch_size:64 (x_train, y_train)
    |> Data.map (fun (x, y) ->
           (x, fun logits -> Loss.cross_entropy_sparse logits y))
  in

  (* Model *)
  let model = Layer.sequential [
    Layer.conv2d ~in_channels:1 ~out_channels:16 ();
    Layer.relu ();
    Layer.max_pool2d ~kernel_size:(2, 2) ();
    Layer.flatten ();
    Layer.linear ~in_features:(16 * 14 * 14) ~out_features:10 ();
  ] in

  (* Trainer *)
  let trainer = Train.make ~model
    ~optimizer:(Optim.adam ~lr:(Optim.Schedule.constant 1e-3) ())
  in
  let st = Train.init trainer ~rngs ~dtype in

  (* Fit with progress reporting *)
  let st = Train.fit trainer st ~rngs
    ~report:(fun ~step ~loss _st ->
      if step mod 100 = 0 then
        Printf.printf "step %d  loss %.4f\n" step loss)
    train_data
  in

  (* Evaluate *)
  let test_acc =
    Metric.eval
      (fun (x, y) ->
        Metric.accuracy (Train.predict trainer st x) y)
      test_batches
  in
  Printf.printf "test accuracy: %.2f%%\n" (test_acc *. 100.)</code></pre>

    <hr>

    <h2>the good parts</h2>

    <p><b>Layers are values.</b>
    A layer is a record with <code>init</code> and <code>apply</code>. Compose them with <code>sequential</code> for homogeneous pipelines or <code>compose</code> for heterogeneous ones (e.g. embeddings to dense layers). Custom layers are just records.</p>

    <p><b>Parameters are data.</b>
    <code>Ptree.t</code> is a tree of tensors with dict and list nodes. Map over all parameters, count them, serialize them to SafeTensors, or load them from HuggingFace. No hidden state.</p>

    <p><b>Data pipelines.</b>
    <code>Data.t</code> is a lazy, composable iterator. Build pipelines from tensors or arrays, shuffle, batch, map, and feed directly to <code>Train.fit</code>. Or consume with <code>fold</code> and <code>iter</code>.</p>

    <p><b>Metrics that compose.</b>
    <code>Metric.eval</code> folds any function over a data pipeline and returns the mean. Accuracy, precision, recall, F1 are plain functions. Track running means with <code>Metric.tracker</code>.</p>

    <hr>

    <h2>what's implemented</h2>

    <div class="feature-grid">
      <div>
        <h3 class="color-red">layers</h3>
        <ul>
          <li>&#10003; Linear (dense)</li>
          <li>&#10003; Conv1d, Conv2d</li>
          <li>&#10003; LayerNorm, RMSNorm, BatchNorm</li>
          <li>&#10003; Embedding (int32 inputs)</li>
          <li>&#10003; Dropout</li>
          <li>&#10003; MaxPool2d, AvgPool2d</li>
          <li>&#10003; Flatten</li>
        </ul>
      </div>
      <div>
        <h3 class="color-red">activations</h3>
        <ul>
          <li>&#10003; ReLU, GELU, SiLU</li>
          <li>&#10003; Tanh, Sigmoid</li>
        </ul>
        <h3 class="color-red">attention</h3>
        <ul>
          <li>&#10003; Multi-head self-attention</li>
          <li>&#10003; Grouped query attention (GQA)</li>
          <li>&#10003; Rotary position embeddings (RoPE)</li>
          <li>&#10003; Causal masking</li>
        </ul>
      </div>
      <div>
        <h3 class="color-red">training</h3>
        <ul>
          <li>&#10003; SGD, Adam, AdamW, RMSprop, Adagrad</li>
          <li>&#10003; LR schedules (cosine, warmup, exponential)</li>
          <li>&#10003; Cross-entropy, BCE, MSE, MAE losses</li>
          <li>&#10003; Accuracy, precision, recall, F1</li>
          <li>&#10003; Gradient clipping</li>
          <li>&#10003; Early stopping</li>
        </ul>
      </div>
      <div>
        <h3 class="color-red">ecosystem</h3>
        <ul>
          <li>&#10003; Data pipelines (shuffle, batch, map)</li>
          <li>&#10003; SafeTensors checkpointing</li>
          <li>&#10003; HuggingFace Hub downloads</li>
          <li>&#10003; MNIST/Fashion-MNIST datasets</li>
          <li>&#10003; Weight initialization (Glorot, He, LeCun)</li>
          <li>&#10003; Context passing for transformers</li>
        </ul>
      </div>
    </div>

    <hr>

    <h2>get started</h2>

    <p>Kaun is part of the Raven ecosystem. When it's released:</p>

    <pre class="language-bash"><code class="language-bash">opam install kaun</code></pre>

    <pre class="language-ocaml"><code class="language-ocaml">open Kaun

let () =
  let rngs = Rune.Rng.key 42 in
  let dtype = Rune.float32 in
  let x = Rune.create dtype [| 4; 2 |] [| 0.; 0.; 0.; 1.; 1.; 0.; 1.; 1. |] in
  let y = Rune.create dtype [| 4; 1 |] [| 0.; 1.; 1.; 0. |] in
  let model = Layer.sequential [
    Layer.linear ~in_features:2 ~out_features:4 ();
    Layer.tanh ();
    Layer.linear ~in_features:4 ~out_features:1 ();
  ] in
  let trainer = Train.make ~model
    ~optimizer:(Optim.adam ~lr:(Optim.Schedule.constant 0.01) ())
  in
  let st = Train.init trainer ~rngs ~dtype in
  let st = Train.fit trainer st ~rngs
    (Data.repeat 1000 (x, fun pred -> Loss.binary_cross_entropy pred y))
  in
  let pred = Train.predict trainer st x |> Rune.sigmoid in
  Printf.printf "XOR predictions: ";
  for i = 0 to 3 do Printf.printf "%.2f " (Rune.item [ i; 0 ] pred) done</code></pre>

    <p>For now, check out the <a href="/docs/kaun/">documentation</a> to learn more.</p>
  </main>

</body>
</html>
