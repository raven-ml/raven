<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>kaun - Neural networks for OCaml | raven</title>
  <link rel="stylesheet" href="/styles.css">
</head>
<body>
  <main class="main-content">
    <nav class="kaun-nav nav-breadcrumb">
      <a href="/">raven</a> / kaun <span class="color-red rune-symbol">ᚲ</span>
      [ <a href="/docs/kaun/">docs</a> |
      <a href="https://github.com/raven-ml/raven/tree/main/kaun">source</a> ]
    </nav>

    <div class="kaun-hero hero">
      <h1>kaun <span style="font-size: 48px; margin-left: -20px; opacity: 0.6;" class="color-red">ᚲ</span></h1>
      <p class="tagline">PyTorch's ease. Flax's modularity. OCaml's type safety.</p>
    </div>

    <hr>

    <h2>why kaun?</h2>

    <div class="feature-grid">
      <div>
        <h3 class="color-red">functional models</h3>
        <p>Models are immutable records. Parameters are data, not hidden state. Everything composes.</p>
      </div>
      <div>
        <h3 class="color-red">type-safe training</h3>
        <p>Catch shape mismatches at compile time. Never debug another runtime dimension error.</p>
      </div>
      <div>
        <h3 class="color-red">built on rune</h3>
        <p>Automatic differentiation built in. Your loss function is just a function.</p>
      </div>
      <div>
        <h3 class="color-red">pure optimizers</h3>
        <p>Optimizers are functions, not stateful objects. Perfect for distributed training.</p>
      </div>
    </div>

    <hr>

    <h2>show me the code</h2>

    <div class="code-compare">
      <div>
        <h4>PyTorch</h4>
        <pre class="language-python"><code class="language-python">import torch
import torch.nn as nn

class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 128)
        self.fc2 = nn.Linear(128, 10)
        
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        return self.fc2(x)

model = MLP()
optimizer = torch.optim.Adam(model.parameters())</code></pre>
      </div>
      <div>
        <h4>KAUN</h4>
        <pre class="language-ocaml"><code class="language-ocaml">open Kaun

(* Model is a record *)
type model = {
  fc1: Linear.t;
  fc2: Linear.t;
}

(* Forward is a function *)
let forward model x =
  x
  |> Linear.forward model.fc1
  |> Activation.relu
  |> Linear.forward model.fc2

(* Initialize *)
let rng = Rng.make 42 in
let model = {
  fc1 = Linear.create rng ~input_dim:784 ~output_dim:128;
  fc2 = Linear.create rng ~input_dim:128 ~output_dim:10;
}</code></pre>
      </div>
    </div>

    <hr>

    <h2>training loop</h2>

    <pre class="language-ocaml"><code class="language-ocaml">(* Loss function *)
let loss_fn model x y =
  let logits = forward model x in
  Loss.sigmoid_binary_cross_entropy ~targets:y logits

(* Get gradients using Rune *)
let loss, grads = value_and_grad loss_fn model x y

(* Update with optimizer *)
let optimizer = Optimizer.adam ~lr:0.001 () in
let model', opt_state' = Optimizer.update optimizer opt_state model grads

(* Pure functional - old model unchanged *)</code></pre>

    <hr>

    <h2>what's implemented</h2>

    <p>Kaun is in early development. Here's what works today:</p>

    <div class="feature-grid">
      <div>
        <h3 class="color-red">layers</h3>
        <ul style="list-style: none; padding: 0;">
          <li>✓ Linear (dense/fully-connected)</li>
          <li>✓ Parameter trees for composition</li>
          <li>⏳ Conv2d, BatchNorm (coming for alpha)</li>
          <li>⏳ Dropout, LayerNorm (coming for alpha)</li>
        </ul>
      </div>
      <div>
        <h3 class="color-red">training</h3>
        <ul style="list-style: none; padding: 0;">
          <li>✓ SGD and Adam optimizers</li>
          <li>✓ Binary cross-entropy loss</li>
          <li>✓ Activation functions (relu, sigmoid, tanh)</li>
          <li>⏳ More losses and metrics (coming for alpha)</li>
        </ul>
      </div>
    </div>

    <hr>

    <h2>design principles</h2>

    <p><b>Models are data.</b> No classes, no inheritance. A model is just a record containing parameters. This makes serialization, inspection, and manipulation trivial.</p>

    <p><b>Training is functional.</b> Optimizers don't mutate state - they return new parameters. This enables techniques like checkpointing and distributed training without special frameworks.</p>

    <p><b>Leverage Rune.</b> We don't reimplement autodiff or device management. Kaun is a thin layer of neural network abstractions over Rune's primitives.</p>

    <hr>

    <h2>get started</h2>

    <p>Kaun isn't released yet. For now, check out the <a href="/docs/kaun/">documentation</a> to learn more.</p>

    <p>When it's ready:</p>

    <pre class="language-bash"><code class="language-bash">opam install kaun</code></pre>

    <pre class="language-ocaml"><code class="language-ocaml">open Kaun

(* XOR problem *)
let x = Tensor.of_float_list [|4; 2|] [0.; 0.; 0.; 1.; 1.; 0.; 1.; 1.]
let y = Tensor.of_float_list [|4; 1|] [0.; 1.; 1.; 0.]

(* Train a model *)
let model = train_xor x y ~epochs:1000</code></pre>
  </main>

</body>
</html>