<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>rune - Automatic differentiation for OCaml | raven</title>
  <link rel="stylesheet" href="/styles.css">
</head>
<body>
  <main class="main-content">
    <nav class="rune-nav nav-breadcrumb">
      <a href="/">raven</a> / rune
      [ <a href="/docs/rune/">docs</a> |
      <a href="https://github.com/raven-ml/raven/tree/main/rune">source</a> ]
    </nav>

    <div class="rune-hero hero">
      <h1>rune</h1>
      <p class="tagline">JAX's transformations. OCaml's guarantees. Differentiable everything.</p>
      <span style="font-size: 36px; opacity: 0.2;" class="color-orange">ᚠ ᚢ ᚦ ᚨ ᚱ ᚲ</span>
    </div>

    <hr>

    <h2>why rune?</h2>

    <div class="feature-grid">
      <div>
        <h3 class="color-orange">composable transformations</h3>
        <p>grad, jit, vmap - they compose like functions should. Build complex ML systems from simple parts.</p>
      </div>
      <div>
        <h3 class="color-orange">effect-based autodiff</h3>
        <p>OCaml 5's effects make automatic differentiation elegant. No tape, no graph - just functions.</p>
      </div>
      <div>
        <h3 class="color-orange">multi-device support</h3>
        <p>Same code runs on CPU or Metal. Device placement is explicit and type-safe.</p>
      </div>
      <div>
        <h3 class="color-orange">type-safe gradients</h3>
        <p>Shape errors at compile time. Device mismatches impossible. Your gradients always match your parameters.</p>
      </div>
    </div>

    <hr>

    <h2>show me the code</h2>

    <div class="code-compare">
      <div>
        <h4>JAX</h4>
        <pre class="language-python"><code class="language-python">import jax
import jax.numpy as jnp
from jax import grad, jit

# Define function
def f(x):
    return jnp.sum(x ** 2)

# Transform it
grad_f = grad(f)
fast_grad_f = jit(grad_f)</code></pre>
      </div>
      <div>
        <h4>RUNE</h4>
        <pre class="language-ocaml"><code class="language-ocaml">open Rune

(* Define function *)
let f x = 
  sum (mul x x)

(* Transform it *)
let grad_f = grad f
let fast_grad_f = jit grad_f</code></pre>
      </div>
    </div>

    <hr>

    <h2>automatic differentiation</h2>

    <p>Rune uses OCaml's effect system to implement autodiff. Write normal functions, get derivatives for free:</p>

    <pre class="language-ocaml"><code class="language-ocaml">(* Any function works *)
let my_function x =
  let y = sin x in
  let z = mul x y in
  sum z

(* Get gradient function *)
let df_dx = grad my_function

(* Compute value and gradient together *)
let value, gradient = value_and_grad my_function x</code></pre>

    <hr>

    <h2>device placement</h2>

    <pre class="language-ocaml"><code class="language-ocaml">(* CPU computation *)
let x = rand cpu Float32 [|100|]

(* Metal GPU (macOS) *)
let gpu = metal () in
let y = rand gpu Float32 [|100|]

(* Operations run on tensor's device *)
let z = add y y  (* runs on GPU *)</code></pre>

    <hr>

    <h2>neural network example</h2>

    <pre class="language-ocaml"><code class="language-ocaml">(* Simple two-layer network *)
let mlp w1 b1 w2 b2 x =
  let h = add (matmul x w1) b1 in
  let h = maximum h (zeros_like h) in  (* ReLU *)
  add (matmul h w2) b2

(* Loss function *)
let loss params x y =
  let [w1; b1; w2; b2] = params in
  let pred = mlp w1 b1 w2 b2 x in
  mean (mul (sub pred y) (sub pred y))

(* Get gradients for all parameters *)
let grad_loss = grads loss

(* Training step *)
let update params x y lr =
  let grads = grad_loss params x y in
  List.map2 
    (fun p g -> sub p (mul (scalar cpu Float32 lr) g))
    params grads</code></pre>

    <hr>

    <h2>what's coming</h2>

    <p>Rune works today for automatic differentiation. Post-v1, we're adding:</p>

    <ul>
      <li><b>JIT to LLVM/Metal/CUDA</b> - Real compilation, not just tracing</li>
      <li><b>vmap</b> - Automatic vectorization over batch dimensions</li>
      <li><b>Forward-mode AD</b> - For Jacobian-vector products</li>
      <li><b>Higher-order derivatives</b> - Hessians and beyond</li>
    </ul>

    <hr>

    <h2>ecosystem</h2>

    <div class="feature-grid">
      <div>
        <h3 class="color-orange">kaun - neural networks</h3>
        <p>High-level neural network library built on Rune. Layers, optimizers, and training loops that just work.</p>
        <p><a href="/kaun/" class="color-red">Learn more →</a></p>
      </div>
      <div>
        <h3 class="color-orange">sowilo - computer vision</h3>
        <p>Differentiable image processing. Every operation supports autodiff.</p>
        <p><a href="/sowilo/" class="color-indigo">Learn more →</a></p>
      </div>
    </div>

    <hr>

    <h2>get started</h2>

    <p>Rune isn't released yet. For now, check out the <a href="/docs/rune/">documentation</a> to learn more.</p>

    <p>When it's ready:</p>

    <pre class="language-bash"><code class="language-bash">opam install rune</code></pre>

    <pre class="language-ocaml"><code class="language-ocaml">open Rune

let () =
  (* Define a function *)
  let f x = sum (mul x x) in

  (* Get its gradient *)
  let grad_f = grad f in

  (* Test it *)
  let x = scalar cpu Float32 3.0 in
  let g = grad_f x in
  Printf.printf "f(3) = %.1f\n" (unsafe_get (f x) [||]);
  Printf.printf "f'(3) = %.1f\n" (unsafe_get g [||])</code></pre>
  </main>

</body>
</html>