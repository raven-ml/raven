<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>brot - Text tokenization for OCaml | raven</title>
  <link rel="stylesheet" href="/styles.css">
</head>
<body>
  <main class="main-content">
    <nav class="brot-nav nav-breadcrumb">
      <a href="/">raven</a> / brot
      [ <a href="/docs/brot/">docs</a> |
      <a href="https://github.com/raven-ml/raven/tree/main/brot">source</a> ]
    </nav>

    <div class="brot-hero hero">
      <h1>brot</h1>
      <p class="tagline">HuggingFace compatible. OCaml fast. Production-ready tokenization.</p>
    </div>

    <hr>

    <h2>why brot?</h2>

    <div class="feature-grid">
      <div>
        <h3 class="color-teal">huggingface compatible</h3>
        <p>Load any tokenizer.json. Same models, same results. Your BERT, GPT-2, and T5 tokenizers work out of the box.</p>
      </div>
      <div>
        <h3 class="color-teal">composable pipeline</h3>
        <p>Five stages, each optional: normalizer, pre-tokenizer, model, post-processor, decoder. Mix and match for any architecture.</p>
      </div>
      <div>
        <h3 class="color-teal">five algorithms</h3>
        <p>BPE, WordPiece, Unigram, word-level, and character-level. Train any of them from scratch on your own data.</p>
      </div>
      <div>
        <h3 class="color-teal">faster than rust</h3>
        <p>1.3-6x faster than HuggingFace's Rust tokenizers library. Pure OCaml, no FFI, no bindings.</p>
      </div>
    </div>

    <hr>

    <h2>show me the code</h2>

    <div class="code-compare">
      <div>
        <h4>HUGGINGFACE TOKENIZERS</h4>
        <pre class="language-python"><code class="language-python">from tokenizers import Tokenizer

# Load pretrained
tokenizer = Tokenizer.from_file(
    "tokenizer.json"
)

# Encode
output = tokenizer.encode("Hello world!")
ids = output.ids

# Decode
text = tokenizer.decode(ids)

# Batch with padding
tokenizer.enable_padding(length=128)
batch = tokenizer.encode_batch(
    ["Hello", "World"]
)</code></pre>
      </div>
      <div>
        <h4>BROT</h4>
        <pre class="language-ocaml"><code class="language-ocaml">open Brot

(* Load pretrained *)
let tokenizer =
  from_file "tokenizer.json"
  |> Result.get_ok

(* Encode *)
let encoding = encode tokenizer "Hello world!"
let ids = Encoding.ids encoding

(* Decode *)
let text = decode tokenizer ids

(* Batch with padding *)
let batch = encode_batch tokenizer
  ~padding:(padding (`Fixed 128))
  ["Hello"; "World"]</code></pre>
      </div>
    </div>

    <hr>

    <h2>the composable pipeline</h2>

    <p>Every tokenizer is a pipeline. Each stage is optional, composable, and serializable to HuggingFace JSON.</p>

    <pre class="language-ocaml"><code class="language-ocaml">(* Build a BERT tokenizer from components *)
let tokenizer =
  wordpiece ~vocab ~unk_token:"[UNK]"
    ~normalizer:(Normalizer.bert ~lowercase:true ())
    ~pre:(Pre_tokenizer.bert ())
    ~post:(Post_processor.bert ~cls:("[CLS]", 2) ~sep:("[SEP]", 3) ())
    ~decoder:(Decoder.wordpiece ())
    ~specials:(List.map special ["[PAD]"; "[UNK]"; "[CLS]"; "[SEP]"])
    ~pad_token:"[PAD]"
    ()

(* Or a GPT-2 tokenizer *)
let tokenizer =
  bpe ~vocab ~merges
    ~pre:(Pre_tokenizer.byte_level ())
    ~post:(Post_processor.byte_level ())
    ~decoder:(Decoder.byte_level ())
    ()</code></pre>

    <hr>

    <h2>rich encoding metadata</h2>

    <p>Every encoding carries alignment metadata. Map tokens back to source text, mask padding, and distinguish sentence pairs.</p>

    <pre class="language-ocaml"><code class="language-ocaml">let enc = encode tokenizer "Hello world!" in
Encoding.ids enc              (* token IDs *)
Encoding.tokens enc           (* token strings *)
Encoding.offsets enc          (* byte spans in source *)
Encoding.attention_mask enc   (* 1=real, 0=padding *)
Encoding.type_ids enc         (* 0/1 for sentence pairs *)
Encoding.special_tokens_mask enc  (* 1=special, 0=content *)
Encoding.word_ids enc         (* source word index *)</code></pre>

    <hr>

    <h2>the good parts</h2>

    <p><b>HuggingFace round-trip</b><br>
    Load tokenizer.json, save tokenizer.json. The same format Python uses. Share tokenizers between OCaml and Python without conversion.</p>

    <p><b>Train from scratch</b><br>
    BPE, WordPiece, Unigram, and word-level training built in. Point at your corpus, get a tokenizer. No Python required.</p>

    <p><b>Batch processing</b><br>
    Padding, truncation, sentence pairs, and overflow handling. Everything you need to feed a transformer, in a single call.</p>

    <p><b>Encode with context</b><br>
    Every token carries its byte offset, word index, type ID, and special-token flag. Map model outputs back to source text without guesswork.</p>

    <hr>

    <h2>what's implemented</h2>

    <div class="feature-grid">
      <div>
        <h3 class="color-teal">algorithms</h3>
        <ul>
          <li>✓ BPE (GPT-2, RoBERTa)</li>
          <li>✓ WordPiece (BERT, DistilBERT)</li>
          <li>✓ Unigram (T5, AlBERT, mBART)</li>
          <li>✓ Word-level</li>
          <li>✓ Character-level</li>
        </ul>
      </div>
      <div>
        <h3 class="color-teal">pipeline stages</h3>
        <ul>
          <li>✓ Normalizers (BERT, NFC/NFD/NFKC/NFKD, byte-level)</li>
          <li>✓ Pre-tokenizers (BERT, byte-level, metaspace, whitespace)</li>
          <li>✓ Post-processors (BERT, RoBERTa, template, byte-level)</li>
          <li>✓ Decoders (BPE, WordPiece, byte-level, metaspace, CTC)</li>
        </ul>
      </div>
      <div>
        <h3 class="color-teal">features</h3>
        <ul>
          <li>✓ Training (BPE, WordPiece, Unigram, word-level)</li>
          <li>✓ Batch encoding with padding/truncation</li>
          <li>✓ Sentence pair encoding</li>
          <li>✓ Overflow handling (sliding window)</li>
          <li>✓ HuggingFace JSON import/export</li>
        </ul>
      </div>
      <div>
        <h3 class="color-teal">encoding metadata</h3>
        <ul>
          <li>✓ Token IDs and strings</li>
          <li>✓ Byte offsets into source text</li>
          <li>✓ Attention masks</li>
          <li>✓ Type IDs (sentence pairs)</li>
          <li>✓ Special token masks</li>
          <li>✓ Word IDs</li>
        </ul>
      </div>
    </div>

    <hr>

    <h2>get started</h2>

    <p>Brot is part of the Raven ecosystem. When it's released:</p>

    <pre class="language-bash"><code class="language-bash">opam install brot</code></pre>

    <pre class="language-ocaml"><code class="language-ocaml">open Brot

let () =
  let tokenizer = from_file "tokenizer.json" |> Result.get_ok in
  let encoding = encode tokenizer "Hello world!" in
  let ids = Encoding.ids encoding in
  Printf.printf "Tokens: %d\n" (Array.length ids);
  let text = decode tokenizer ids in
  print_endline text</code></pre>

    <p>For now, check out the <a href="/docs/brot/">documentation</a> to learn more.</p>
  </main>

</body>
</html>
